<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css"
        integrity="sha384-B0vP5xmATw1+K9KRQjQERJvTumQW0nPEzvF6L/Z6nronJ3oUOFUFpCjEUQouq2+l" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

    <title>RAD: Retrieval-Augmented Monocular Metric Depth Estimation for Underrepresented Classes</title>

    <style>
        p {
            line-height: 1.2;
        }

        .justify {
            text-align: justify;
        }

        /* .abstract {
            line-height: 1;
        } */

        .teaser {
            position: relative;
            display: inline;
        }

        .teaser .img-top {
            position: absolute;
            top: 0;
            left: 0;
            opacity: 0;
        }

        .teaser:hover .img-top {
            opacity: 1;
        }

        .tab {
            margin-left: 40px;
        }

        .card {
            border: 0px;
            margin: 5px;
        }

        .img-teaser {
            height: 150px;
        }

        .emph {
            font-weight: bold;
        }

        .emph2 {
            font-style: italic;
        }

        .question {
            font-weight: bold;
        }

        .carousel {
            width: 640px;
            height: 370px;
        }

        .carousel-img {
            display: block;
            margin-left: auto;
            margin-right: auto;
        }

        .carousel-caption {
            color: black;
            position: relative;
            left: auto;
            right: auto;
            margin: 10px;

        }

        .carousel-item {
            margin: auto;
        }

        .carousel-control-next,
        .carousel-control-prev,
        .carousel-indicators {
            filter: invert(100%);
        }

        .carousel-indicators {
            /* margin: 10px; */
        }

        .paper-snapshot {
            box-shadow: 10px 10px 5px grey;
        }

        .stroop-img {
            /* box-shadow: 10px 10px 5px grey; */
            border: 1px solid grey;
            margin: 20px;
            width: 95%
        }

        @media screen and (min-width: 800px) {

            .paper-snapshot {
                position: relative;
                top: 40px;
                left: 10px;
            }

            .stroop-img {
                position: relative;
                top: 15px;
            }
        }

        .paper-snapshot-link {
            max-width: 200px;
            margin-left: auto;
            margin-right: auto;
        }

        .card-img-top {
            /* width: 100%; */
            /* height: 15vw; */
            object-fit: contain;
        }

        .ncd-img {
            height: 100px;
            /* width: auto; */
        }

        .ncd-card-body {
            /* text-align: center; */
            font-size: 18px;
            font-family: 'Lucida Console', monospace;
            position: relative;
            left: 20px;
        }

        .colorbox {
            width: 15%;
            height: 15%;
            border: 2px solid black;
            margin: 5px;
        }

        .ncd-card-first {
            position: relative;
            left: 40px;
            top: 5px;
        }
    </style>
</head>

<body class="container" style="max-width:840px">

    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-Piv4xVNRyMGpqkS2by6br4gNJ7DXjqk09RmUpJ8jgGtD7zP9yug3goQfGII0yAns"
        crossorigin="anonymous"></script>

    <!-- heading -->
    <div>

        <!-- title -->
        <div class='row mt-5 mb-3'>
            <div class='col text-center'>
                <p class="h2 font-weight-normal">RAD: Retrieval-Augmented Monocular Metric Depth Estimation for Underrepresented Classes</p>
            </div>
        </div>

        <!-- authors -->
        <div class="col text-center h6 font-weight-bold mb-2 ">
            <span><a class="col-md-4 col-xs-6 pb-2" href="https://www.linkedin.com/in/michael-baltaxe-5146bb4?originalSubdomain=il">Michael Baltaxe<sup>1, 2</sup></a></span>
            <span><a class="col-md-4 col-xs-6 pb-2" href="https://sites.google.com/view/danlevi/home">Dan Levi<sup>1</sup></a></span>
            <span><a class="col-md-4 col-xs-6 pb-2" href="https://sagiebenaim.github.io">Sagie Benaim<sup>2</sup></a></span>
        </div>

        <!-- affiliations -->
        <div class='row mb-1'>
            <div class='col text-center'>
                <p class="h6">
                    <a><span><sup>1</sup>General Motors, <sup>2</sup>Hebrew University of Jerusalem</span></a>
                </p>
            </div>
        </div>
    </div>
    
    <div class="text-center mt-3">
    	<div class="btn-group" role="group" aria-label="Top menu">
        	<a class="btn btn-primary" href="https://arxiv.org/abs/2602.09532">Paper</a>
    	</div>
	</div>

    <!-- Paper section -->
    <div>
        <hr>
        <div class="row">
            <div class='col-md-12 col-sm-12 col-xs-12 align-middle mt-1'>
                <p class='h4 font-weight-bold'>Abstract</p>
                <p class='abstract justify'>
                    Monocular Metric Depth Estimation (MMDE) is essential for physically intelligent systems, yet accurate depth estimation for underrepresented classes in complex scenes remains a persistent challenge. To address this, we propose RAD, a retrieval-augmented framework that approximates the benefits of multi-view stereo by utilizing retrieved neighbors as structural geometric proxies. Our method first employs an uncertainty-aware retrieval mechanism to identify low-confidence regions in the input and retrieve RGB-D context samples containing semantically similar content. We then process both the input and retrieved context via a dual-stream network and fuse them using a matched cross-attention module, which transfers geometric information only at reliable point correspondences. Evaluations on NYU Depth v2, KITTI, and Cityscapes demonstrate that RAD significantly outperforms state-of-the-art baselines on underrepresented classes, reducing relative absolute error by 29.2% on NYU Depth v2, 13.3% on KITTI, and 7.2% on Cityscapes, while maintaining competitive performance on standard in-domain benchmarks.
                </p>
            </div>
        </div>
    </div>
    <div>
        <hr>
        <div class="row">
            <div class='col-md-12 col-sm-12 col-xs-12 align-middle mt-1'>
                <p class='h4 font-weight-bold'>Method</p>
                <p class='abstract justify'>
                    Given an input image, RAD (using
DepthAnything v2 backbone) retrieves context views for
highly uncertain objects of underrepresented classes (e.g., candles)
to serve as structural geometric proxies. These are used as part of a
dual-stream network to output an accurate monocular metric depth
estimation, in comparison to the direct baseline of DepthAnything
v2, fixing uncertain regions.
                    <br>
					<center>
                    <img src="teaser.jpg" alt="Teaser" width="720" height="444">
					</center>
                    <br>
                    <p class='h6 font-weight-bold'>Pipeline</p>
                    Given an input image, a set of con-
text samples is sourced using either uncertainty aware
image retrieval (both at training and inference) or 3D augmenta-
tion (only during training). Subsequently, spatial correspondences
are established. These are used to infer depth via a
dual-stream depth estimation network employing matched cross-
attention. Blue blocks indicate components used for
training and inference, while the green block is only for training.
					<br>
					<center>
                    <img src="system_architecture.jpg" alt="System architecture" width="720" height="444">
					</center>
                    <br>
                    <p class='h6 font-weight-bold'>Uncertainty-aware retrieval flow</p>
                    Pixel-wise depth
uncertainty is calculated in parallel to image segmentation. We use
these to keep only highly uncertain segments, masking the rest of
the image. Given the masked image we retrieve relevant examples
from the context/training set using DINO descriptors.
					<br>
					<center>
                    <img src="uncertainty_aware_retrieval.jpg" alt="Uncertainty-aware retrieval" width="720" height="444">
					</center>
				<br>
                <p class='h6 font-weight-bold'>Matched Cross-attention</p>
                (a) illustrates the modified attention architecture designed to enable effective information transfer from the context stream to the input stream. For each
token j in the input image, with query vector Qi[j], attention is
computed using key and value matrices formed by concatenating
the input’s keys (Ki) and values (Vi) with the matched context
keys (Km(j)) and values (Vm(j)). These matched matrices are
constructed by selecting j’s matching context tokens from the full
context matrices Kc and Vc, respectively. (b) shows that matching
tokens are defined as those located within a spatial neighborhood
surrounding the matched point of jin the context image.
            </div>
        </div>
    </div>
	<div>
        <hr>
        <div class="row">
            <div class='col-md-12 col-sm-12 col-xs-12 align-middle mt-1'>
                <p class='h4 font-weight-bold'>Results</p>
                    <p class='h6 font-weight-bold'>Underrepresented classes evaluation</p>
					<br>
					<center>
					<img src="underrepresented_table.jpg" alt="Results free space" width="70%">
                    </center>
					<br>
					<br>
                    <p class='h6 font-weight-bold'>All-classes evaluation</p>
                    <br>
                    <center>
					<img src="all_table.jpg" alt="Results depth" width="80%">
                    </center>
                    <br>
					<br>
                    <p class='h6 font-weight-bold'>Qualitative results</p>
                    <br>
                    <center>
					<img src="qualitative.jpg" alt="Results depth" width="80%">
                    </center>
            </div>
        </div>
    </div>
    <div>
        <hr>
        <div class="row">
            <div class='col-md-12 col-sm-12 col-xs-12 align-middle mt-1'>
                <p class='h4 font-weight-bold'>Citation</p>
                <textarea id="bibtex" class="form-control" rows=6 readonly>
@article{baltaxe2026rad,
  title={RAD: Retrieval-Augmented Monocular Metric Depth Estimation for Underrepresented Classes},
  author={Michael Baltaxe and Dan Levi and Sagie Benaim},
  journal={arXiv},
  year={2026}
}</textarea>
            </div>
        </div>
    </div>
</body>

</html>
